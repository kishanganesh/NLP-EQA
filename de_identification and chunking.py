# -*- coding: utf-8 -*-
"""De-identification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lQf8Ky1si7-5Qms9di27ZbcOPZnr7_yA
"""

import nltk
from nltk import word_tokenize, pos_tag, ne_chunk
import re
import pandas as pd

# Download required NLTK data
nltk.download('punkt')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('averaged_perceptron_tagger')

import re
import pandas as pd
from nltk import ne_chunk, pos_tag, word_tokenize, sent_tokenize
from nltk.tree import Tree

def deidentify_text(text):
    # Remove all quotes and asterisks from the text
    text = text.replace('"', '').replace('*', '')

    tree = ne_chunk(pos_tag(word_tokenize(text)))
    deidentified_text = ''

    # Adding a simple rule to identify and de-identify dates
    date_pattern = re.compile(r'\b(\d{1,2}/\d{1,2}/\d{2,4})\b')

    for subtree in tree:
        if type(subtree) == Tree:
            label = subtree.label()
            if label == 'PERSON':
                deidentified_text += '<PERSON> '
            elif label == 'GPE':
                deidentified_text += '<LOCATION> '
            else:
                for leaf in subtree.leaves():
                    word, _ = leaf
                    if date_pattern.match(word):
                        deidentified_text += '<DATE> '
                    else:
                        deidentified_text += word + ' '
        else:
            word, tag = subtree
            if date_pattern.match(word):
                deidentified_text += '<DATE> '
            else:
                deidentified_text += word + ' '

    return deidentified_text.strip()

def chunk_text(text, max_words=400):
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_word_count = 0

    for sentence in sentences:
        words = word_tokenize(sentence)
        word_count = len(words)

        # If the current sentence is too long, we split it into smaller parts
        if word_count > max_words:
            long_sentence_parts = [' '.join(words[i:i+max_words]) for i in range(0, word_count, max_words)]
            chunks.extend(long_sentence_parts)
        else:
            if current_word_count + word_count <= max_words:
                current_chunk.append(sentence)
                current_word_count += word_count
            else:
                chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_word_count = word_count

    # Add the last chunk if it's not empty
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

# Here's where you can place your input text directly
input_text = """"<input text>
"""
# De-identify the text
deidentified_text = deidentify_text(input_text)

# Output the deidentified text

print(deidentified_text)

# Chunk the text
text_chunks = chunk_text(deidentified_text, 400)

# Create identifiers and order numbers
document_identifiers = [f'doc_{i+1}' for i in range(len(text_chunks))]
order_numbers = [i+1 for i in range(len(text_chunks))]

# Create a DataFrame to hold the document text, identifier, and order
df = pd.DataFrame({
    'document_identifier': document_identifiers,
    'document_text': text_chunks,
    'order': order_numbers
})

# Save the DataFrame to a CSV file
df.to_csv('document_text_with_identifiers_and_order.csv', index=False)

print(df)